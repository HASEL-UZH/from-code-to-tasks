{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18c54a1b5c61a5e7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import statistics\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.698446Z",
     "start_time": "2023-10-18T10:59:18.696114Z"
    }
   },
   "id": "8df5dad73f50bcec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Embedding Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a1e388fabe5c5da"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def generate_embeddings():\n",
    "    embeddings_object = {\"code_emb\": {}, \"pr_emb\": {}}\n",
    "    commit_data_folder_path = \"../0_data_collection/datasets/commit_data_removed_empty_and_only_comments\"\n",
    "    if os.path.exists(commit_data_folder_path) and os.path.isdir(commit_data_folder_path):\n",
    "        for folder_name in os.listdir(commit_data_folder_path):\n",
    "            folder_path = os.path.join(commit_data_folder_path, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                commit_info_path = os.path.join(folder_path, \"commit_info.json\")\n",
    "                if os.path.exists(commit_info_path):\n",
    "                    with open(commit_info_path, \"r\") as json_file:\n",
    "                        commit_info_data = json.load(json_file)\n",
    "                        pull_request_value = commit_info_data.get(\"pull request\")\n",
    "                        pull_request_embedding = generate_pull_request_embedding(pull_request_value).detach().numpy()\n",
    "                        # Save the pull_request_embedding as an npy file in the same folder\n",
    "                        pull_request_npy_file_path = os.path.join(folder_path, \"pull_request_summation_embedding.npy\")\n",
    "                        np.save(pull_request_npy_file_path, pull_request_embedding)\n",
    "                        embeddings_object[\"pr_emb\"][folder_name] = pull_request_npy_file_path  # Store the npy file path\n",
    "                        # Get code diff embedding\n",
    "                        diff_file_paths = []\n",
    "                        for file in os.listdir(folder_path):\n",
    "                            if file.endswith(\".diff\"):\n",
    "                                diff_file_path = os.path.join(folder_path, file)\n",
    "                                diff_file_paths.append(diff_file_path)\n",
    "                        added_code, deleted_code = split_code_diff(diff_file_paths)\n",
    "                        code_embedding = generate_code_diff_embedding(added_code, deleted_code).detach().numpy()\n",
    "                        # Save the code_embedding as an npy file in the same folder\n",
    "                        code_npy_file_path = os.path.join(folder_path, \"code_summation_embedding.npy\")\n",
    "                        np.save(code_npy_file_path, code_embedding)\n",
    "                        embeddings_object[\"code_emb\"][folder_name] = code_npy_file_path  # Store the npy file path\n",
    "    print(embeddings_object)\n",
    "    return embeddings_object"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T11:05:28.996845Z",
     "start_time": "2023-10-18T11:05:28.989169Z"
    }
   },
   "id": "2eebcca2a0bec7cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarity Object Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be7f734f8be540fd"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def generate_similarity_object():\n",
    "    similarity_array = []\n",
    "\n",
    "    commit_data_folder = \"../0_data_collection/datasets/commit_data_removed_empty_and_only_comments\"\n",
    "    sliding_window_folder = \"../0_data_collection/datasets/commit_data_sliding_window\"\n",
    "\n",
    "    for sliding_window_subfolder in os.listdir(sliding_window_folder):\n",
    "        sliding_window_subfolder_path = os.path.join(sliding_window_folder, sliding_window_subfolder)\n",
    "        similarity_dict = {}\n",
    "\n",
    "        for folder_code_diff in os.listdir(sliding_window_subfolder_path):\n",
    "            folder_path_code_diff = os.path.join(commit_data_folder, folder_code_diff)\n",
    "            code_embedding_file = os.path.join(folder_path_code_diff, \"code_summation_embedding.npy\")\n",
    "            if os.path.exists(code_embedding_file):\n",
    "                code_embedding = np.load(code_embedding_file)\n",
    "            similarity_dict[folder_code_diff] = {}\n",
    "\n",
    "            for folder_pull_request in os.listdir(sliding_window_subfolder_path):\n",
    "                folder_path_pull_request = os.path.join(commit_data_folder, folder_pull_request)\n",
    "                pull_request_embedding_file = os.path.join(folder_path_pull_request, \"pull_request_summation_embedding.npy\")\n",
    "                if os.path.exists(pull_request_embedding_file):\n",
    "                    pull_request_embedding = np.load(pull_request_embedding_file)\n",
    "                cosine_similarity = calculate_cosine_similarity(code_embedding, pull_request_embedding)\n",
    "                similarity_dict[folder_code_diff][folder_pull_request]= cosine_similarity\n",
    "\n",
    "        similarity_array.append(similarity_dict)\n",
    "    return similarity_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.707539Z",
     "start_time": "2023-10-18T10:59:20.706044Z"
    }
   },
   "id": "a320eb6a3f704a17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pull Request Embedding Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "801941a115c33847"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def generate_pull_request_embedding(pull_request):\n",
    "    # Tokenize the task descriptions\n",
    "    task_description_tokens = tokenizer.tokenize(pull_request)\n",
    "   # Adding CLS token, SEP token and EOS token\n",
    "    tokens = [tokenizer.cls_token]+task_description_tokens+[tokenizer.eos_token]\n",
    "    #Convert tokens to IDs\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens[1:])\n",
    "   # Create embeddings\n",
    "    task_description_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "    summed_embeddings = torch.sum(task_description_embeddings, dim=1)\n",
    "    return summed_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.720292Z",
     "start_time": "2023-10-18T10:59:20.708980Z"
    }
   },
   "id": "372cae84deba87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code Diff Embedding Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ecf35566f3b7d5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_code_diff_embedding(added_code, deleted_code, max_seq_length=512):\n",
    "    added_tokens = tokenizer.tokenize(added_code, truncation=True)\n",
    "    deleted_tokens = tokenizer.tokenize(deleted_code, truncation=True)\n",
    "    # Truncate tokens if the total length exceeds the maximum sequence length\n",
    "    total_tokens = len(added_tokens) + len(deleted_tokens) + 3  # 3 for [CLS], [SEP], [EOS]\n",
    "    if total_tokens > max_seq_length:\n",
    "        # Calculate how many tokens to keep for each part (added, deleted)\n",
    "        keep_added_tokens = int((max_seq_length - 3) / 2)\n",
    "        keep_deleted_tokens = max_seq_length - 3 - keep_added_tokens\n",
    "        # Truncate tokens accordingly\n",
    "        added_tokens = added_tokens[:keep_added_tokens]\n",
    "        deleted_tokens = deleted_tokens[:keep_deleted_tokens]\n",
    "    # Adding CLS token, SEP token and EOS token\n",
    "    tokens = [tokenizer.cls_token] + added_tokens + [tokenizer.sep_token] + deleted_tokens + [tokenizer.eos_token]\n",
    "\n",
    "    # Convert tokens to IDs\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens[1:])\n",
    "    # Create embeddings\n",
    "    code_embeddings = model(torch.tensor(tokens_ids)[None, :])[0]\n",
    "    summed_embeddings = torch.sum(code_embeddings, dim=1)\n",
    "    return summed_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.720576Z",
     "start_time": "2023-10-18T10:59:20.714193Z"
    }
   },
   "id": "f64414bf1e3a833b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cosine Similarity Calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2d492b4c2cf3b26"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(code_embedding, task_description_embedding):\n",
    "    code_embedding_np = code_embedding.reshape(1, -1)\n",
    "    task_description_embedding_np = task_description_embedding.reshape(1, -1)\n",
    "    similarity = cosine_similarity(code_embedding_np, task_description_embedding_np)\n",
    "    similarity_value = similarity[0, 0]\n",
    "    return similarity_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.720762Z",
     "start_time": "2023-10-18T10:59:20.716828Z"
    }
   },
   "id": "be4c8ff9b04f3cd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation Metric Calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0a702ba5d0ebfb7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(k, similarity_objects):\n",
    "    \n",
    "    top_k_accuracies = []\n",
    "    for sample_object in similarity_objects:\n",
    "        total_embeddings = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # Iterate through each key in the sample_object\n",
    "        for key in sample_object:\n",
    "            # Get the top k highest scores\n",
    "            highest_scores = sorted(sample_object[key].values(), reverse=True)[:k]\n",
    "            same = sample_object[key][key]\n",
    "\n",
    "            # Check if the correct prediction is among the top k highest scores\n",
    "            if same in highest_scores:\n",
    "                correct_predictions += 1\n",
    "            total_embeddings += 1\n",
    "\n",
    "        # Calculate top-k accuracy and append to the top_k_accuracies list\n",
    "        top_k_accuracy = correct_predictions / total_embeddings\n",
    "        top_k_accuracies.append(top_k_accuracy)\n",
    "    \n",
    "    mean_value = statistics.mean(top_k_accuracies)\n",
    "    median_value = statistics.median(top_k_accuracies)\n",
    "    stdev_value = statistics.stdev(top_k_accuracies)\n",
    "    min_value = min(top_k_accuracies)\n",
    "    max_value = max(top_k_accuracies)\n",
    "    evaluation_metrics = {\n",
    "                'mean': mean_value,\n",
    "                'median': median_value,\n",
    "                'stdev': stdev_value,\n",
    "                'min': min_value,\n",
    "                'max': max_value\n",
    "            }\n",
    "    \n",
    "    return evaluation_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.734874Z",
     "start_time": "2023-10-18T10:59:20.721276Z"
    }
   },
   "id": "d6a3e8194cdb62f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save Evaluation Metrics to CSV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c5eab51deffb448"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def save_evaluation_metrics_to_csv(title, description, date, evaluation_metrics):\n",
    "    header = ['Title', 'Description', 'Date','Mean', 'Median', 'Standard Deviation', 'Minimum', 'Maximum']\n",
    "    data = [[title, description, date, evaluation_metrics['mean'], evaluation_metrics['median'],evaluation_metrics['stdev'], evaluation_metrics['min'], evaluation_metrics['max']]]\n",
    "\n",
    "    # Check if the '4_results' folder exists, create it if not\n",
    "    if not os.path.exists('../4_results'):\n",
    "        os.makedirs('../4_results')\n",
    "\n",
    "    # Define the file path for the 'results.csv' file in '4_results' folder\n",
    "    file_path = os.path.join('../4_results', 'results.csv')\n",
    "\n",
    "\n",
    "    # Check if the file exists, if not create it and write the header\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)  # Write the header if the file is created\n",
    "\n",
    "        # Write the data row\n",
    "        writer.writerows(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.735094Z",
     "start_time": "2023-10-18T10:59:20.724434Z"
    }
   },
   "id": "277b2a38e4667914"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split Code Diff into added lines and deleted lines"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397bf093fb674f7f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def split_code_diff(diff_files):\n",
    "    added_lines = \"\"\n",
    "    deleted_lines = \"\"\n",
    "    for diff_file in diff_files:\n",
    "        with open(diff_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('+'):\n",
    "                    added_lines += line[1:].strip() + '\\n'\n",
    "                elif line.startswith('-'):\n",
    "                    deleted_lines += line[1:].strip() + '\\n'\n",
    "    return added_lines, deleted_lines"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:20.735166Z",
     "start_time": "2023-10-18T10:59:20.727683Z"
    }
   },
   "id": "231d818d6e1a8e83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Similarity Manager"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af9563283af95f33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Model and Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8391ba217894f79d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:22.323625Z",
     "start_time": "2023-10-18T10:59:20.730033Z"
    }
   },
   "id": "3e9d363ef91a9a89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result Meta-Data Specification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "860f050f0e13469a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "title = 'codebert summation run k=1'\n",
    "description = 'codebert summation run k=1'\n",
    "date = datetime.datetime.now().strftime('%d.%m.%y (%H:%M:%S)')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T10:59:22.325653Z",
     "start_time": "2023-10-18T10:59:22.324178Z"
    }
   },
   "id": "d94283d8ab957b3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Embedding Generation "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dbeb7044252c950"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m embeddings_object \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 25\u001B[0m, in \u001B[0;36mgenerate_embeddings\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m         diff_file_paths\u001B[38;5;241m.\u001B[39mappend(diff_file_path)\n\u001B[1;32m     24\u001B[0m added_code, deleted_code \u001B[38;5;241m=\u001B[39m split_code_diff(diff_file_paths)\n\u001B[0;32m---> 25\u001B[0m code_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_code_diff_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43madded_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeleted_code\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Save the code_embedding as an npy file in the same folder\u001B[39;00m\n\u001B[1;32m     27\u001B[0m code_npy_file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(folder_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_summation_embedding.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 19\u001B[0m, in \u001B[0;36mgenerate_code_diff_embedding\u001B[0;34m(added_code, deleted_code, max_seq_length)\u001B[0m\n\u001B[1;32m     17\u001B[0m tokens_ids \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(tokens[\u001B[38;5;241m1\u001B[39m:])\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Create embeddings\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m code_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens_ids\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     20\u001B[0m summed_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(code_embeddings, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m summed_embeddings\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    835\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    837\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    838\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    839\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    842\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    843\u001B[0m )\n\u001B[0;32m--> 844\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    856\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    857\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:529\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    520\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    521\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    522\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    526\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    527\u001B[0m     )\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 529\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    534\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    536\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    540\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    403\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    410\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    412\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 413\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    420\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    332\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    338\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    339\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 340\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    349\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    350\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Box Sync/Home/UZH/Bachelorarbeit/bachelor-thesis/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:236\u001B[0m, in \u001B[0;36mRobertaSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    233\u001B[0m     past_key_value \u001B[38;5;241m=\u001B[39m (key_layer, value_layer)\n\u001B[1;32m    235\u001B[0m \u001B[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001B[39;00m\n\u001B[0;32m--> 236\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelative_key\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelative_key_query\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    239\u001B[0m     query_length, key_length \u001B[38;5;241m=\u001B[39m query_layer\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], key_layer\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_object = generate_embeddings()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T11:04:05.094518Z",
     "start_time": "2023-10-18T10:59:22.326802Z"
    }
   },
   "id": "f5843387c207b37c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarity Object Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41d4c73c6d2b202c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_objects = generate_similarity_object()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T11:04:05.099730Z",
     "start_time": "2023-10-18T11:04:05.098994Z"
    }
   },
   "id": "867e0a7a161e2911"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation Metric Calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b6898abe33b4b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 1\n",
    "evaluation_metrics = calculate_evaluation_metrics(k, similarity_objects)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-18T11:04:05.099219Z"
    }
   },
   "id": "a0d597e4cbe240ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save Evaluation Metrics to CSV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18625777d302626c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_evaluation_metrics_to_csv(title, description, date, evaluation_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-18T11:04:05.099256Z"
    }
   },
   "id": "c6c311faec8ac123"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
