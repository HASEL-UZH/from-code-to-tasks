{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:22.632166Z",
     "start_time": "2023-09-19T14:47:22.618872Z"
    }
   },
   "id": "ca720e6b9082bfeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embeddings Code Diff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32f7453c6615a9b2"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def create_code_embeddings_added_deleted(codebert_model, added_code, deleted_code):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    \n",
    "    # Tokenize the added and deleted code\n",
    "    added_tokens = tokenizer.tokenize(added_code)\n",
    "    deleted_tokens = tokenizer.tokenize(deleted_code)\n",
    "    print(added_tokens)\n",
    "    print(deleted_tokens)\n",
    "\n",
    "    # Adding CLS token, SEP token and EOS token\n",
    "    tokens = [tokenizer.cls_token]+added_tokens+[tokenizer.sep_token]+deleted_tokens+[tokenizer.eos_token]\n",
    "    print(tokens)\n",
    "    \n",
    "    #Convert tokens to IDs\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens[1:])\n",
    "    print(tokens_ids)\n",
    "    \n",
    "    # Create embeddings\n",
    "    code_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "    \n",
    "    print(code_embeddings)\n",
    "\n",
    "    return code_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:22.634406Z",
     "start_time": "2023-09-19T14:47:22.622661Z"
    }
   },
   "id": "a26edf4fd1102d9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embeddings Code Diff (Sum)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8f1a1c1237c21d6"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def create_code_embeddings_added_deleted_sum(codebert_model, added_code, deleted_code):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    \n",
    "    # Tokenize the added and deleted code\n",
    "    added_tokens = tokenizer.tokenize(added_code)\n",
    "    deleted_tokens = tokenizer.tokenize(deleted_code)\n",
    "    print(added_tokens)\n",
    "    print(deleted_tokens)\n",
    "\n",
    "    # Adding CLS token, SEP token and EOS token\n",
    "    tokens = [tokenizer.cls_token]+added_tokens+[tokenizer.sep_token]+deleted_tokens+[tokenizer.eos_token]\n",
    "    print(tokens)\n",
    "    \n",
    "    #Convert tokens to IDs\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens[1:])\n",
    "    print(tokens_ids)\n",
    "    \n",
    "    # Create embeddings\n",
    "    code_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "    summed_embeddings = torch.sum(code_embeddings, dim=1)\n",
    "    \n",
    "    print(summed_embeddings)\n",
    "\n",
    "    return summed_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:22.635572Z",
     "start_time": "2023-09-19T14:47:22.626771Z"
    }
   },
   "id": "f4ebbad0d0fe4ab5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embeddings Task Description"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6285886997599c9"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def create_task_description_embeddings (codebert_model, task_description):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    \n",
    "    # Tokenize the task descriptions\n",
    "    task_description_tokens = tokenizer.tokenize(task_description)\n",
    "    print(task_description_tokens)\n",
    "\n",
    "    # Adding CLS token, SEP token and EOS token\n",
    "    tokens = [tokenizer.cls_token]+task_description_tokens+[tokenizer.eos_token]\n",
    "    print(tokens)\n",
    "    \n",
    "    #Convert tokens to IDs\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens[1:])\n",
    "    print(tokens_ids)\n",
    "    \n",
    "    # Create embeddings\n",
    "    task_description_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "\n",
    "    print(task_description_embeddings)\n",
    "\n",
    "    return task_description_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:22.645259Z",
     "start_time": "2023-09-19T14:47:22.630510Z"
    }
   },
   "id": "8625a75e93d05f5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embeddings Task Description Sum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b77a64198a603d6c"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def create_task_description_embeddings_sum (codebert_model, task_description):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    \n",
    "    # Tokenize the task descriptions\n",
    "    task_description_tokens = tokenizer.tokenize(task_description)\n",
    "    print(task_description_tokens)\n",
    "\n",
    "    # Adding CLS token, SEP token and EOS token\n",
    "    tokens = [tokenizer.cls_token]+task_description_tokens+[tokenizer.eos_token]\n",
    "    print(tokens)\n",
    "    \n",
    "    #Convert tokens to IDs\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens[1:])\n",
    "    print(tokens_ids)\n",
    "    \n",
    "    # Create embeddings\n",
    "    task_description_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "    summed_embeddings = torch.sum(task_description_embeddings, dim=1)\n",
    "\n",
    "    print(summed_embeddings)\n",
    "\n",
    "    return summed_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:22.654869Z",
     "start_time": "2023-09-19T14:47:22.632560Z"
    }
   },
   "id": "db1ca1f7c2f4acbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cosine Similarity Calculation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36d8b97e62b71321"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(code_embedding, task_description_embedding):\n",
    "\n",
    "    size_code_embedding = code_embedding.size()\n",
    "    size_task_description_embedding = task_description_embedding.size()\n",
    "\n",
    "    # truncate if necessary\n",
    "    if size_code_embedding.numel() < size_task_description_embedding.numel():\n",
    "        task_description_embedding = task_description_embedding[:, :size_code_embedding[1], :]\n",
    "\n",
    "    elif size_code_embedding.numel() > size_task_description_embedding.numel():\n",
    "        code_embedding = code_embedding[:, :size_task_description_embedding[1], :]\n",
    "\n",
    "    # calculate cosine similarity\n",
    "    code_embedding_np = code_embedding.detach().numpy().reshape(1, -1)\n",
    "    task_description_embedding_np = task_description_embedding.detach().numpy().reshape(1, -1)\n",
    "\n",
    "    similarity = cosine_similarity(code_embedding_np, task_description_embedding_np)\n",
    "    similarity_value = similarity[0, 0]\n",
    "    return similarity_value\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:22.655655Z",
     "start_time": "2023-09-19T14:47:22.638356Z"
    }
   },
   "id": "ff3dbc67d74e1a26"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ess', '.', 'broad', 'cast', 'Message', '(\"', 'ess', 'entials', '.', 'ban', 'ip', '.', 'not', 'ify', '\",', 'Ġt', 'l', '(\"', 'player', 'Ban', 'I', 'p', 'Address', '\",', 'Ġsender', 'Name', ',', 'Ġip', 'Address', ',', 'Ġban', 'Reason', '));']\n",
      "['ess', '.', 'broad', 'cast', 'Message', '(\"', 'ess', 'entials', '.', 'ban', '.', 'not', 'ify', '\",', 'Ġt', 'l', '(\"', 'player', 'Ban', 'I', 'p', 'Address', '\",', 'Ġsender', 'Name', ',', 'Ġip', 'Address', ',', 'Ġban', 'Reason', '));']\n",
      "['<s>', 'ess', '.', 'broad', 'cast', 'Message', '(\"', 'ess', 'entials', '.', 'ban', 'ip', '.', 'not', 'ify', '\",', 'Ġt', 'l', '(\"', 'player', 'Ban', 'I', 'p', 'Address', '\",', 'Ġsender', 'Name', ',', 'Ġip', 'Address', ',', 'Ġban', 'Reason', '));', '</s>', 'ess', '.', 'broad', 'cast', 'Message', '(\"', 'ess', 'entials', '.', 'ban', '.', 'not', 'ify', '\",', 'Ġt', 'l', '(\"', 'player', 'Ban', 'I', 'p', 'Address', '\",', 'Ġsender', 'Name', ',', 'Ġip', 'Address', ',', 'Ġban', 'Reason', '));', '</s>']\n",
      "[3361, 4, 32990, 5182, 42394, 46469, 3361, 40288, 4, 7384, 1588, 4, 3654, 4591, 1297, 326, 462, 46469, 23233, 33809, 100, 642, 46486, 1297, 40792, 31723, 6, 36180, 46486, 6, 2020, 48147, 48749, 2, 3361, 4, 32990, 5182, 42394, 46469, 3361, 40288, 4, 7384, 4, 3654, 4591, 1297, 326, 462, 46469, 23233, 33809, 100, 642, 46486, 1297, 40792, 31723, 6, 36180, 46486, 6, 2020, 48147, 48749, 2]\n",
      "tensor([[[-0.0654,  0.2517, -0.0832,  ..., -0.2780, -0.4066,  0.5197],\n",
      "         [-0.5488,  0.0479,  0.2150,  ..., -0.0742, -0.3760,  0.2273],\n",
      "         [-0.6992,  0.4416,  0.5202,  ..., -0.4823, -0.3231,  0.6309],\n",
      "         ...,\n",
      "         [-0.3874,  0.5565,  0.7415,  ..., -0.8657, -0.2836,  0.5329],\n",
      "         [-0.2573, -0.2952,  0.5368,  ..., -0.5037, -0.5859,  0.6686],\n",
      "         [-0.0652,  0.2520, -0.0826,  ..., -0.2777, -0.4069,  0.5197]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "['Sil', 'ent', 'Ġban', 'Ġmessages', '?']\n",
      "['<s>', 'Sil', 'ent', 'Ġban', 'Ġmessages', '?', '</s>']\n",
      "[23719, 1342, 2020, 3731, 116, 2]\n",
      "tensor([[[-0.1260,  0.3668,  0.0517,  ..., -0.1206, -0.3243,  0.3357],\n",
      "         [-0.1888,  0.0405,  0.4487,  ..., -0.6255, -0.2984,  0.3847],\n",
      "         [-0.0327,  0.0146,  0.1116,  ..., -0.1886, -0.3787,  0.3327],\n",
      "         [ 0.0646, -0.2576,  0.0333,  ..., -0.2046, -0.4412,  0.5130],\n",
      "         [-0.3993, -0.0463,  0.3261,  ..., -0.6539, -0.3462,  0.6148],\n",
      "         [-0.1259,  0.3673,  0.0525,  ..., -0.1205, -0.3250,  0.3363]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "['Red', 'uce', 'Ġpermission', 'Ġcheck', 'Ġcalls', 'Ġin', 'ĠPlayer', 'Command', 'Send', 'Event']\n",
      "['<s>', 'Red', 'uce', 'Ġpermission', 'Ġcheck', 'Ġcalls', 'Ġin', 'ĠPlayer', 'Command', 'Send', 'Event', '</s>']\n",
      "[15638, 15431, 5537, 1649, 1519, 11, 8251, 46785, 42735, 44879, 2]\n",
      "tensor([[[-0.1067,  0.2948,  0.0211,  ..., -0.1465, -0.3591,  0.3352],\n",
      "         [ 0.0649,  0.1938,  0.3285,  ..., -1.2587, -0.0341,  0.5278],\n",
      "         [ 0.1101, -0.0531, -0.1223,  ...,  0.4389, -0.6509,  0.6087],\n",
      "         ...,\n",
      "         [-0.1207,  0.2514, -0.3339,  ..., -0.5432, -0.2946,  0.3135],\n",
      "         [ 0.0040, -0.0212, -0.1889,  ..., -0.2576, -0.3915,  0.5950],\n",
      "         [-0.1063,  0.2951,  0.0216,  ..., -0.1461, -0.3597,  0.3353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Cosine similarity correct: 0.6874274\n",
      "Cosine similarity incorrect: 0.7684398\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "added_code = 'ess.broadcastMessage(\"essentials.banip.notify\", tl(\"playerBanIpAddress\", senderName, ipAddress, banReason));'\n",
    "deleted_code = 'ess.broadcastMessage(\"essentials.ban.notify\", tl(\"playerBanIpAddress\", senderName, ipAddress, banReason));'\n",
    "task_description_correct = \"Silent ban messages?\"\n",
    "task_description_incorrect = \"Reduce permission check calls in PlayerCommandSendEvent\"\n",
    "\n",
    "code_embedding = create_code_embeddings_added_deleted(model, added_code, deleted_code)\n",
    "task_description_embedding_correct = create_task_description_embeddings(model, task_description_correct)\n",
    "task_description_embedding_incorrect = create_task_description_embeddings(model, task_description_incorrect)\n",
    "cosine_similarity_correct = calculate_cosine_similarity(code_embedding, task_description_embedding_correct)\n",
    "cosine_similarity_incorrect = calculate_cosine_similarity(code_embedding, task_description_embedding_incorrect)\n",
    "print(\"Cosine similarity correct:\", cosine_similarity_correct)\n",
    "print(\"Cosine similarity incorrect:\", cosine_similarity_incorrect)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:24.349631Z",
     "start_time": "2023-09-19T14:47:22.641636Z"
    }
   },
   "id": "5c2ff7645e073769"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T14:47:24.350058Z",
     "start_time": "2023-09-19T14:47:24.348298Z"
    }
   },
   "id": "e9e63ad40cdf8f46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3.9",
   "language": "python",
   "display_name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
